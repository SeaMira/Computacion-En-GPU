CPU: no son suficientemente rápidos, ni siquiera los multicore
Necesidad de resolver grandes problemas, con mucha info en una cantidad razonable de tiempo, con un hardware moderado y de computación paralela dio paso a la
GPU. 
Capaz de manejar miles de hilos en paralelo,  cada uno de ellos no de gran poder como los de un core en la CPU. Se consigue un gran speedup en GPU al compararla con la CPU.
speedup:an increase in speed, especially in a person's or machine's rate of working

Sin embargo, no todos los problemas son paralelizables, ej, los time
-dependent, que necesitan de resultados previamente calculados (recursión, el cálculo de sqrt(x) usando el método de newton rhapson). Este tipo de problemas es mejor resolverlos en CPU.

Problemas no paralelizables:
--Algoritmos secuenciales: Un ejemplo concreto sería el algoritmo de cifrado RSA. En este algoritmo, cada paso de cifrado o descifrado depende del resultado del paso anterior, lo que lo hace intrínsecamente secuencial. Dado que cada operación depende de los resultados anteriores, no es posible paralelizar eficientemente este proceso.

--Procesamiento de datos en serie: Imagina un sistema de control de inventario en una tienda. Para realizar un seguimiento preciso de los productos, es necesario procesar las ventas y las reposiciones de manera secuencial, ya que el estado actual del inventario depende de transacciones pasadas. Si se intentara paralelizar este proceso, podría haber problemas de coherencia de datos y pérdida de precisión en el inventario.

--Problemas que involucran dependencias de datos complejas: Un ejemplo sería el proceso de planificación de rutas logísticas para una empresa de entrega. En este caso, hay una red de rutas, puntos de entrega y restricciones de tiempo que están altamente interconectados. Cada decisión sobre una ruta puede afectar a múltiples rutas futuras, lo que hace que sea difícil dividir el proceso en tareas independientes y paralelizarlo de manera efectiva sin comprometer la calidad de la planificación.

Embarrassingly parallel problems:
--Análisis de datos en paralelo: Muchas aplicaciones de análisis de datos, como la clasificación de grandes conjuntos de datos o el procesamiento de imágenes, pueden dividirse fácilmente en tareas independientes que pueden ejecutarse simultáneamente en múltiples núcleos o procesadores.

--Renderizado de imágenes: Generar imágenes a partir de modelos tridimensionales o realizar operaciones gráficas complejas, como el renderizado de efectos especiales en películas, es un ejemplo clásico de un problema "embarrassingly parallel", ya que cada píxel de la imagen final puede ser calculado de forma independiente.

--Simulaciones Monte Carlo: Muchas simulaciones numéricas, como las utilizadas en finanzas, física o biología, pueden dividirse en numerosas iteraciones independientes que pueden ejecutarse en paralelo, lo que hace que sean altamente paralelizables y, por lo tanto, aptas para ejecutar en GPUs.

--simulación de eventos, integración numérica, transformada de fourier donde cada armónico se calcula por separado, ataques de fuerza bruta en criptografía, entre otros.


Una relación cercana a la computación en paralelo es la que existe con el hardware que la sostiene y el modelo de programacón. 



##################### CONCEPTOS BÁSICOS ########################
Concurrency is a property of a program (at design level) where two or
more tasks can be in progress simultaneously. Se enfoca en la estructura de un programa como un conjunto de tareas que pueden ejecutarse en un orden no predeterminado, sin asumir nada sobre la ejecución simultánea.

Parallelism is a run-time property where two or more tasks are being
executed simultaneously. múltiples tareas o partes de una tarea en múltiples procesadores o núcleos dentro de un sistema de computación.

El paralelismo se considera incluido en concurrencia porque, para que múltiples tareas se ejecuten en paralelo, primero deben ser concurrentes; es decir, deben ser independientes o capaces de progresar simultáneamente. El paralelismo es un caso específico de concurrencia donde la ejecución simultánea es una realidad física, aprovechando los recursos de hardware disponibles.

jemplo de problema resoluble de manera concurrente:
Problema eficiente para la concurrencia: Servidor web que maneja múltiples solicitudes de usuarios.

Concurrencia: Un servidor web maneja solicitudes entrantes de diferentes usuarios. Cada solicitud puede ser atendida por un proceso o hilo separado, permitiendo que el servidor trabaje en múltiples solicitudes "al mismo tiempo" sin necesariamente procesarlas en paralelo. Este enfoque es eficiente porque mejora la capacidad de respuesta del servidor, permitiendo que maneje solicitudes de forma asincrónica.
Ejemplo de problema no eficiente para concurrencia o paralelismo:
Problema ineficiente: Cálculo de Fibonacci utilizando recursión directa.

Ineficiencia: La naturaleza secuencial y dependiente de este cálculo, especialmente en su implementación recursiva simple, lo hace inadecuado para la concurrencia y el paralelismo, ya que cada término depende del cálculo del término anterior.
Kernel para el problema del servidor web:
Dado que el problema del servidor web es más un escenario de sistema que un algoritmo específico, un "kernel" en este contexto podría referirse a un pseudocódigo básico para manejar solicitudes de manera asincrónica. Sin embargo, para un ejemplo más concreto, consideremos un problema computacional paralelo: el procesamiento de una imagen para convertirla a escala de grises.

Parámetros de entrada:

ImagenColor: Una matriz 3D representando los píxeles de la imagen en color, donde las dimensiones son altura, anchura, y canales de color (RGB).
Pseudocódigo:

plaintext
Copy code
kernel ConvertirAEscalaDeGrises(ImagenColor):
    i, j = IdentificarPosicionDelHiloEnLaImagen()
    pixelRGB = ImagenColor[i][j]
    gris = 0.299*pixelRGB[0] + 0.587*pixelRGB[1] + 0.114*pixelRGB[2]
    ImagenGris[i][j] = gris
Salida:

ImagenGris: Una matriz 2D donde cada elemento representa un píxel en escala de grises de la imagen original.
Este kernel puede ejecutarse en paralelo para cada píxel de la imagen, aprovechando la capacidad de procesamiento de un GPU, donde cada hilo calcula la conversión de un píxel RGB a escala de grises de forma independiente.


################ COMPUTACIÓN PARALELA #####################
Acto de resolver un problema de tamaño n al dividir el dominio en k >2  partes y resolviendolas con p procesadores físicos simultáneamente. 

Problema de Paralelismo de Datos
En un problema de paralelismo de datos, el mismo conjunto de operaciones (o kernel) se aplica a diferentes elementos de un conjunto de datos. La clave aquí es que las operaciones son independientes entre sí y pueden ejecutarse simultáneamente sin necesidad de comunicación o sincronización entre ellas.

Ejemplo: Multiplicación de Matrices. La multiplicación de dos matrices es un ejemplo clásico de un problema de paralelismo de datos. Para calcular un elemento en la matriz resultante, se realiza una operación que implica solo una fila de la primera matriz y una columna de la segunda matriz. Estas operaciones son independientes unas de otras y pueden realizarse en paralelo para todos los elementos de la matriz resultante.

Parámetros de entrada: Dos matrices, A y B.
Operación: Para cada elemento C[i][j]
C[i][j] en la matriz resultante C, calcula la suma de los productos de los elementos correspondientes en la fila i de la matriz A y la columna j de la matriz B.
Salida: Una nueva matriz C que es el producto de A y B.
Problema de Paralelismo de Tareas
En un problema de paralelismo de tareas, diferentes tareas (o procesos), que pueden ser diferentes operaciones, se ejecutan en paralelo. Cada tarea puede trabajar en una parte diferente del problema o en un problema completamente diferente, y pueden requerir comunicación o sincronización entre ellas.

Ejemplo: Procesamiento de Transacciones en un Sistema Bancario. Considera un sistema bancario en línea donde las transacciones de los clientes se procesan en paralelo. Cada transacción (retiro, depósito, transferencia, etc.) es una tarea independiente y puede requerir acceso a diferentes cuentas o servicios. Aunque las transacciones son independientes, pueden requerir sincronización para garantizar la coherencia de los datos (por ejemplo, asegurando que los balances de las cuentas se actualicen de manera atómica).

Parámetros de entrada: Un conjunto de transacciones bancarias, cada una con su tipo y los datos necesarios para su procesamiento.
Operaciones: Cada transacción se procesa de acuerdo a su tipo; por ejemplo, actualizar el balance de una cuenta, transferir fondos entre cuentas, etc.
Salida: Actualización de los balances de las cuentas y registros de las transacciones procesadas.

Data-parallel problems are ideal candidates for the GPU. The reason is because the
GPU architecture works best when all threads execute the same instructions but on different data. On the other hand, task-parallel problems are best suited for the CPU. The reason is because the CPU architecture allows different tasks to be executed on each thread.
This classification scheme is critical for achieving the best partition of the problem domain, which is in fact the first step when designing a parallel algorithm. It also provides
useful information when choosing the best hardware for the implementation (CPU or
GPU). Computational physics problems often classify as data-parallel, therefore they are
good candidates for a massive parallelization on GPU. Since the aim of this work is to
provide a survey on parallel computing for computational physics, most of the explanations will be in the context of data-parallel problems.

################### MEDIDAS DE RENDIMIENTO #####################
Set de métricas para cuantificar la calidad de un algoritmo. Para algortmos secuenciales tiempo y espacio eran suficientes. En caso de algoritmos paralelos el speedup y la eficiencia también son necesarias para estudiar su calidad y la mejoría. En el lado experimental además se tienen medidas como el ancho de banda de memoria y los FLOPS (floating point operations per second).

Dado un problema de tamaño n, el tiempo de ejecución de un algoritmo paralelo usando p procesadores se denomina:
T(n,p)

El work y el span son la base para las métricas de eficiencia y speedup. Ambas mediciones son importantes porque dan límites a la computación en paralelo.
work: T(n,1) es el tiempo total necesitado para ejecutar un algoritmo paralelo usando solo un procesador, denominado T(n,1). Es decir, el tiempo que se demora el algortimo secuencial.

span: T(n,00)la mayor cantidad de tiempo que se necesita para ejecutar el algoritmo usando infinitos procesadores

Ley del trabajo T(n,p)>= T(n,1)/p : se demorará por lo menos 1/8 del trabajo. Algoritmos paralelos corren mejor cuando el trabajo por procesador está balanceado

Ley del span T(n,p) >= T(n, 00) un algoritmo paralelo no se demorará menos de lo que se demora la mínima cantidad de tiempo en que un procesador termine su trabajo en una máquina de ifninitos procesadores.

El speedup: mide cuán rápido es el algoritmo paralelo vs el mejor algoritmo secuencial con una cantidad de procesadores p S_p = T_s(n,1)/T(n,p)



Work y Span son conceptos fundamentales en la teoría de la computación paralela para analizar el rendimiento de algoritmos paralelos.

Work (W): Representa el total de tiempo o cantidad de esfuerzo computacional requerido para completar una tarea si se ejecutara en un solo procesador. Es la suma de todos los pasos de computación realizados por el algoritmo. En otras palabras, es el tiempo de ejecución del algoritmo en un entorno secuencial sin paralelismo.

Span (S), también conocido como profundidad crítica, es el tiempo que toma ejecutar el algoritmo en un número infinito de procesadores, es decir, el tiempo que toma completar la tarea más larga desde el inicio hasta el fin, considerando que todas las tareas paralelas se ejecutan instantáneamente. Representa el camino más largo de dependencias en la ejecución del algoritmo.

Utilidad: Estos conceptos se utilizan para calcular la eficiencia, el speedup y la escalabilidad de algoritmos paralelos. Ayudan a identificar cuánto puede mejorarse el rendimiento mediante paralelización y cuál es el límite teórico de esta mejora. Work y Span permiten calcular el speedup teórico máximo usando la Ley de Brent, que afirma que el tiempo de ejecución T_p con p procesadores es al menos 
W/p+S, dando una medida de cuán bien un algoritmo puede ser paralelizado.

Diferencia/Relación entre 
T(n,1) y T_s(n,1)

T(n,1) representa el tiempo que toma ejecutar un algoritmo paralelo en un solo procesador, es decir, sin aprovechar el paralelismo. Este tiempo incluye todo el overhead introducido por el diseño paralelo del algoritmo, incluso cuando se ejecuta secuencialmente.

T_s(n,1), por otro lado, es el tiempo de ejecución del mejor algoritmo secuencial conocido para el mismo problema. Este tiempo es típicamente el más optimizado posible para una ejecución en un solo procesador, sin considerar estructuras de paralelismo.

Diferencia/Relación: La principal diferencia entre 
T(n,1) y T_s(n,1) radica en que T(n,1) puede ser mayor que T_s(n,1) debido al overhead adicional de paralelización que se presenta incluso en la ejecución en un solo núcleo. T_s(n,1) es una medida de eficiencia para el algoritmo secuencial óptimo, mientras que T(n,1) refleja la eficiencia de un algoritmo paralelo cuando se ve forzado a operar en un entorno secuencial.

Modelos de Speedup y sus Características Principales
Los tres modelos de speedup (escalado fijo, escalado por tamaño, y tiempo fijo) reflejan diferentes maneras de entender y medir cómo el paralelismo afecta el rendimiento.

Fixed-size speedup (Escalado fijo): Evalúa cómo cambia el tiempo de ejecución cuando se aumenta el número de procesadores mientras se mantiene fijo el tamaño del problema. Su característica principal es medir directamente el efecto del paralelismo en la velocidad de ejecución para un problema de tamaño constante.

Scaled speedup (Escalado por tamaño): Aumenta el tamaño del problema proporcionalmente al número de procesadores. Esto refleja escenarios donde la cantidad de trabajo crece con los recursos disponibles, manteniendo constante la carga de trabajo por procesador. Su característica principal es medir la capacidad de un sistema para manejar eficientemente problemas más grandes a medida que se dispone de más procesadores.

Fixed-time speedup (Tiempo fijo): Consiste en ajustar el tamaño del problema de manera que el tiempo de ejecución se mantenga constante mientras se aumenta el número de procesadores. Este modelo es menos común pero útil para evaluar cómo diferentes configuraciones de procesadores pueden resolver problemas de mayor tamaño en el mismo tiempo.


El "linear speedup" se refiere al ideal en la computación paralela donde duplicar el número de procesadores (o unidades de procesamiento) resulta en una reducción a la mitad del tiempo de ejecución del programa. Es decir, si un programa se ejecuta en un tiempo 
�
T usando 
�
N procesadores, el objetivo sería que se ejecute en un tiempo 
�
/
2
T/2 usando 
2
�
2N procesadores, logrando así un speedup lineal.

¿Por qué se cree que es alcanzable?
Independencia de tareas: En problemas donde las tareas son completamente independientes y no requieren comunicación entre ellas, teóricamente es posible asignar una tarea por procesador, resultando en un speedup lineal. Esto se aplica bien a problemas "embarazosamente paralelos".

Eficiencia de la paralelización: Para ciertos problemas y algoritmos, especialmente aquellos diseñados con el paralelismo en mente desde el principio, se pueden minimizar los cuellos de botella y la necesidad de sincronización o comunicación, acercándose al speedup lineal.

Uso óptimo de recursos: Con una gestión eficaz de los recursos de hardware y una planificación de tareas que minimice el tiempo de inactividad de los procesadores, algunos creen que el speedup lineal es un objetivo alcanzable.

¿Por qué se cree que no es alcanzable?
Ley de Amdahl: La ley de Amdahl establece que el speedup de un programa está limitado por la porción del programa que debe ejecutarse secuencialmente. Incluso una pequeña parte del código que no se puede paralelizar puede limitar significativamente el speedup total alcanzable, haciendo que el speedup lineal sea prácticamente imposible para muchos programas.

Sobrecostos de paralelización: La paralelización introduce sobrecostos adicionales, como la comunicación entre procesadores, sincronización, y el manejo de datos compartidos. Estos sobrecostos pueden aumentar con el número de procesadores, reduciendo el speedup efectivo.

Problemas de escalabilidad: A medida que aumenta el número de procesadores, la gestión eficiente de la comunicación y la distribución de tareas se vuelve más compleja. Problemas como el desbalance de carga, donde algunos procesadores están inactivos mientras otros están sobrecargados, pueden impedir el speedup lineal.

Limitaciones de hardware y software: Las arquitecturas de hardware y los sistemas operativos tienen limitaciones prácticas en cuanto al paralelismo que pueden gestionar eficazmente. Los cuellos de botella en el acceso a memoria, la latencia de la red en sistemas distribuidos, y la coherencia de caché son factores que pueden limitar el speedup.

Conclusión
El speedup lineal es un ideal hacia el cual se esfuerzan los diseñadores de sistemas paralelos, pero en la práctica, es difícil de alcanzar para muchos problemas debido a las limitaciones inherentes de la computación paralela y la ley de Amdahl. Sin embargo, para problemas específicos altamente paralelizables y con una adecuada optimización, se puede aproximar a este ideal. La búsqueda del speedup lineal impulsa la innovación en el diseño de algoritmos paralelos y la arquitectura de sistemas computacionales.

Speedup medido con la Ley de Amdahl
La Ley de Amdahl se utiliza para predecir el speedup teórico máximo de un proceso al paralelizarlo, enfocándose en la porción del programa que no se puede paralelizar. La fórmula básica es:

S(p)=  1/ [(1−P)+ P/p]
donde:

S(p) es el speedup teórico máximo al usar 
p procesadores.

P es la proporción del programa que se puede paralelizar.

1−P es la proporción que debe ejecutarse secuencialmente.

Ejemplo con la Ley de Amdahl:

Problema: Consideremos el caso de un algoritmo de ordenamiento que necesita ordenar una gran cantidad de datos. Supongamos que el 95% del algoritmo se puede paralelizar (por ejemplo, usando ordenamiento por mezcla paralelo), mientras que el 5% restante (como la combinación final de subconjuntos ordenados) debe ejecutarse secuencialmente.

Si aplicamos la Ley de Amdahl para calcular el speedup teórico al usar 16 procesadores (p=16), tenemos:

S(16) = 1 / [(1−0.95)+0.95/16] ≈ 5.263

Esto significa que, bajo las condiciones ideales descritas por la Ley de Amdahl, podríamos esperar un speedup máximo de aproximadamente 5.26 veces más rápido que la versión secuencial, sin importar cuántos procesadores adicionales utilicemos.

Speedup medido con la Ley de Gustafson
La Ley de Gustafson propone una perspectiva diferente, sugiriendo que el tamaño del problema puede escalar con el número de procesadores, ofreciendo un enfoque más realista para muchas aplicaciones prácticas. La fórmula de Gustafson es:

S′(p)=p−α(p−1)

donde:

S′(p) es el speedup teórico según Gustafson al usar p procesadores.

α es la fracción del tiempo que se dedica a las secciones secuenciales del algoritmo.

p es el número de procesadores.

Ejemplo con la Ley de Gustafson:

Problema: Supongamos que tenemos una aplicación de simulación de clima que puede adaptarse para utilizar más recursos computacionales efectivamente al aumentar el número de procesadores, permitiéndonos hacer simulaciones más detalladas o cubrir áreas geográficas más amplias.

Si consideramos que el 10% del tiempo de ejecución es inherente a tareas secuenciales (α=0.10) y decidimos ejecutar nuestra simulación usando 100 procesadores (p=100), aplicando la Ley de Gustafson obtenemos:

S′(100) = 100 − 0.10*(100 − 1) = 100 − 9.9 = 90.1

Esto sugiere que, ajustando el tamaño del problema al número de procesadores disponibles, podemos esperar que la aplicación se ejecute hasta 90.1 veces más rápido que en un solo procesador, según la Ley de Gustafson.

Ambos ejemplos ilustran cómo las leyes de Amdahl y Gustafson ofrecen perspectivas valiosas sobre el speedup teórico en la computación paralela, cada una desde un ángulo diferente, reflejando la importancia de considerar tanto la capacidad de paralelización del problema como la escalabilidad del tamaño del problema con respecto a los recursos computacionales disponibles.


Las arquitecturas de GPU (Unidad de Procesamiento Gráfico) están diseñadas para procesar grandes cantidades de datos en paralelo, lo que las hace excepcionalmente buenas para gráficos y cálculos científicos que pueden ser paralelizados. Sin embargo, maximizar el rendimiento de la memoria en una GPU puede ser más desafiante que en una CPU (Unidad de Procesamiento Central) por varias razones:

Dependencia del Problema
El rendimiento de la memoria en las GPUs es altamente dependiente del problema específico y de cómo se organizan y acceden los datos. Para obtener el máximo rendimiento, es crucial que las estructuras de datos estén alineadas con las características de la memoria de la GPU, permitiendo que se lean o escriban grandes bloques de datos simultáneamente.

Accesos a Memoria
Los accesos irregulares a la memoria, donde los patrones de lectura y escritura no siguen una secuencia predecible, pueden disminuir significativamente el rendimiento. Esto se debe a que las GPUs prefieren patrones de acceso a memoria coalescidos, donde múltiples hilos de ejecución leen o escriben en direcciones de memoria contiguas en un solo ciclo de reloj.

Alineación de Datos
La alineación de datos se refiere a la organización de los datos en memoria de manera que coincidan con los límites naturales de la misma. En GPUs, esto puede significar asegurarse de que las estructuras de datos comiencen en direcciones que sean múltiplos de ciertos tamaños (por ejemplo, 32 bytes), lo que facilita el acceso coalescido a la memoria.

Tamaño de los Datos
Los tamaños de los bloques de datos que se procesan también juegan un papel importante. Bloques de tamaño inadecuado pueden llevar a un desperdicio de recursos de memoria y a una disminución en el rendimiento debido a la necesidad de realizar más accesos a memoria de lo necesario.

Caché L2 en Arquitecturas Fermi y Kepler
Las arquitecturas Fermi y Kepler de Nvidia introdujeron mejoras significativas en la gestión de la memoria, incluyendo una caché L2 para la memoria global. La caché L2 ayuda a mitigar los problemas de accesos irregulares a la memoria y la desalineación de datos, actuando como un buffer que puede almacenar datos temporalmente y servir múltiples accesos a datos cercanos sin requerir accesos adicionales a la memoria global, que es más lenta.

Arquitecturas de GPU
Las arquitecturas de GPU, como Fermi, Kepler, y sus sucesoras Maxwell, Pascal, Volta, Turing, y Ampere, han sido diseñadas por Nvidia, cada una con mejoras en el rendimiento, eficiencia energética, y capacidades de procesamiento en paralelo. Estas arquitecturas han evolucionado para soportar no solo gráficos computacionales sino también cálculos científicos y de inteligencia artificial, ofreciendo una amplia gama de aplicaciones. Cada nueva generación ha traído mejoras en la jerarquía de memoria, la cantidad de núcleos de procesamiento, y características específicas como unidades de tensor para cálculos de IA, todos diseñados para mejorar el rendimiento y la eficiencia en una amplia gama de cargas de trabajo.



############### MODELOS DE COMPUTACION #################
El modelo PRAM (Parallel Random Access Machine) es un modelo teórico utilizado para el diseño y análisis de algoritmos paralelos. Tiene varias variantes que se diferencian en cómo manejan el acceso concurrente a la memoria:

EREW (Exclusive Read Exclusive Write): En esta variante, ningún procesador puede leer o escribir simultáneamente en la misma ubicación de memoria. Garantiza la exclusividad tanto en lecturas como en escrituras.

CREW (Concurrent Read Exclusive Write): Permite que múltiples procesadores lean de la misma ubicación de memoria simultáneamente, pero solo un procesador puede escribir en una ubicación de memoria en un momento dado.

ERCW (Exclusive Read Concurrent Write): Los procesadores pueden leer exclusivamente, pero varios procesadores pueden escribir en la misma ubicación de memoria al mismo tiempo. Esta variante es menos común debido a los desafíos que presenta la escritura concurrente.

CRCW (Concurrent Read Concurrent Write): Permite lecturas y escrituras concurrentes en la misma ubicación de memoria. Esta es la variante más poderosa pero también la más compleja de manejar debido a los conflictos potenciales de escritura.

Ejemplos para Variantes del Modelo PRAM
CREW - Búsqueda Binaria Paralela: Un buen ejemplo de uso para el modelo CREW es la búsqueda binaria en un arreglo ordenado. Dado que la búsqueda binaria divide el espacio de búsqueda a la mitad en cada paso, se puede asignar a cada procesador un segmento único del arreglo para leer y determinar en qué segmento se debe continuar la búsqueda en el siguiente paso. Aquí, múltiples procesadores leen de diferentes partes del arreglo (lectura concurrente), pero solo uno actualizará el índice de búsqueda (escritura exclusiva).

Entrada: Un arreglo ordenado A de tamaño n y un valor objetivo x.
Proceso: Divide el espacio de búsqueda entre todos los procesadores disponibles. Cada procesador lee su segmento asignado para determinar si contiene el valor objetivo. Se actualiza el rango de búsqueda basado en los resultados de todos los procesadores.
Salida: La posición de x en A o una indicación de que x no está presente.
CRCW - Suma de Prefijos: La suma de prefijos es un algoritmo que puede implementarse eficientemente bajo el modelo CRCW. El objetivo es calcular un nuevo arreglo donde cada elemento en la posición i es la suma de todos los elementos desde la posición 0 hasta i en el arreglo original. En el modelo CRCW, múltiples procesadores pueden trabajar en calcular sumas parciales simultáneamente y escribir sus resultados en el mismo momento, utilizando técnicas como la suma en árbol paralelo.

Entrada: Un arreglo A de n elementos.
Proceso: Cada procesador calcula sumas parciales basadas en la posición y los resultados de otros procesadores, aprovechando el acceso concurrente para lectura y escritura, facilitando la combinación de resultados.
Salida: Un nuevo arreglo donde cada posición i contiene la suma de los elementos de A[0] a A[i].
Estos ejemplos ilustran cómo las distintas variantes del modelo PRAM pueden ser utilizadas para implementar algoritmos paralelos de manera eficiente, tomando en cuenta las restricciones específicas de acceso a la memoria de cada variante.