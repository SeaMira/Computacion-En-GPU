CPU: no son suficientemente rápidos, ni siquiera los multicore
Necesidad de resolver grandes problemas, con mucha info en una cantidad razonable de tiempo, con un hardware moderado y de computación paralela dio paso a la
GPU. 
Capaz de manejar miles de hilos en paralelo,  cada uno de ellos no de gran poder como los de un core en la CPU. Se consigue un gran speedup en GPU al compararla con la CPU.
speedup:an increase in speed, especially in a person's or machine's rate of working

Sin embargo, no todos los problemas son paralelizables, ej, los time
-dependent, que necesitan de resultados previamente calculados (recursión, el cálculo de sqrt(x) usando el método de newton rhapson). Este tipo de problemas es mejor resolverlos en CPU.

Problemas no paralelizables:
--Algoritmos secuenciales: Un ejemplo concreto sería el algoritmo de cifrado RSA. En este algoritmo, cada paso de cifrado o descifrado depende del resultado del paso anterior, lo que lo hace intrínsecamente secuencial. Dado que cada operación depende de los resultados anteriores, no es posible paralelizar eficientemente este proceso.

--Procesamiento de datos en serie: Imagina un sistema de control de inventario en una tienda. Para realizar un seguimiento preciso de los productos, es necesario procesar las ventas y las reposiciones de manera secuencial, ya que el estado actual del inventario depende de transacciones pasadas. Si se intentara paralelizar este proceso, podría haber problemas de coherencia de datos y pérdida de precisión en el inventario.

--Problemas que involucran dependencias de datos complejas: Un ejemplo sería el proceso de planificación de rutas logísticas para una empresa de entrega. En este caso, hay una red de rutas, puntos de entrega y restricciones de tiempo que están altamente interconectados. Cada decisión sobre una ruta puede afectar a múltiples rutas futuras, lo que hace que sea difícil dividir el proceso en tareas independientes y paralelizarlo de manera efectiva sin comprometer la calidad de la planificación.

Embarrassingly parallel problems:
--Análisis de datos en paralelo: Muchas aplicaciones de análisis de datos, como la clasificación de grandes conjuntos de datos o el procesamiento de imágenes, pueden dividirse fácilmente en tareas independientes que pueden ejecutarse simultáneamente en múltiples núcleos o procesadores.

--Renderizado de imágenes: Generar imágenes a partir de modelos tridimensionales o realizar operaciones gráficas complejas, como el renderizado de efectos especiales en películas, es un ejemplo clásico de un problema "embarrassingly parallel", ya que cada píxel de la imagen final puede ser calculado de forma independiente.

--Simulaciones Monte Carlo: Muchas simulaciones numéricas, como las utilizadas en finanzas, física o biología, pueden dividirse en numerosas iteraciones independientes que pueden ejecutarse en paralelo, lo que hace que sean altamente paralelizables y, por lo tanto, aptas para ejecutar en GPUs.

--simulación de eventos, integración numérica, transformada de fourier donde cada armónico se calcula por separado, ataques de fuerza bruta en criptografía, entre otros.


Una relación cercana a la computación en paralelo es la que existe con el hardware que la sostiene y el modelo de programacón. 



##################### CONCEPTOS BÁSICOS ########################
Concurrency is a property of a program (at design level) where two or
more tasks can be in progress simultaneously. Se enfoca en la estructura de un programa como un conjunto de tareas que pueden ejecutarse en un orden no predeterminado, sin asumir nada sobre la ejecución simultánea.

Parallelism is a run-time property where two or more tasks are being
executed simultaneously. múltiples tareas o partes de una tarea en múltiples procesadores o núcleos dentro de un sistema de computación.

El paralelismo se considera incluido en concurrencia porque, para que múltiples tareas se ejecuten en paralelo, primero deben ser concurrentes; es decir, deben ser independientes o capaces de progresar simultáneamente. El paralelismo es un caso específico de concurrencia donde la ejecución simultánea es una realidad física, aprovechando los recursos de hardware disponibles.

jemplo de problema resoluble de manera concurrente:
Problema eficiente para la concurrencia: Servidor web que maneja múltiples solicitudes de usuarios.

Concurrencia: Un servidor web maneja solicitudes entrantes de diferentes usuarios. Cada solicitud puede ser atendida por un proceso o hilo separado, permitiendo que el servidor trabaje en múltiples solicitudes "al mismo tiempo" sin necesariamente procesarlas en paralelo. Este enfoque es eficiente porque mejora la capacidad de respuesta del servidor, permitiendo que maneje solicitudes de forma asincrónica.
Ejemplo de problema no eficiente para concurrencia o paralelismo:
Problema ineficiente: Cálculo de Fibonacci utilizando recursión directa.

Ineficiencia: La naturaleza secuencial y dependiente de este cálculo, especialmente en su implementación recursiva simple, lo hace inadecuado para la concurrencia y el paralelismo, ya que cada término depende del cálculo del término anterior.
Kernel para el problema del servidor web:
Dado que el problema del servidor web es más un escenario de sistema que un algoritmo específico, un "kernel" en este contexto podría referirse a un pseudocódigo básico para manejar solicitudes de manera asincrónica. Sin embargo, para un ejemplo más concreto, consideremos un problema computacional paralelo: el procesamiento de una imagen para convertirla a escala de grises.

Parámetros de entrada:

ImagenColor: Una matriz 3D representando los píxeles de la imagen en color, donde las dimensiones son altura, anchura, y canales de color (RGB).
Pseudocódigo:

plaintext
Copy code
kernel ConvertirAEscalaDeGrises(ImagenColor):
    i, j = IdentificarPosicionDelHiloEnLaImagen()
    pixelRGB = ImagenColor[i][j]
    gris = 0.299*pixelRGB[0] + 0.587*pixelRGB[1] + 0.114*pixelRGB[2]
    ImagenGris[i][j] = gris
Salida:

ImagenGris: Una matriz 2D donde cada elemento representa un píxel en escala de grises de la imagen original.
Este kernel puede ejecutarse en paralelo para cada píxel de la imagen, aprovechando la capacidad de procesamiento de un GPU, donde cada hilo calcula la conversión de un píxel RGB a escala de grises de forma independiente.


################ COMPUTACIÓN PARALELA #####################
Acto de resolver un problema de tamaño n al dividir el dominio en k >2  partes y resolviendolas con p procesadores físicos simultáneamente. 

Problema de Paralelismo de Datos
En un problema de paralelismo de datos, el mismo conjunto de operaciones (o kernel) se aplica a diferentes elementos de un conjunto de datos. La clave aquí es que las operaciones son independientes entre sí y pueden ejecutarse simultáneamente sin necesidad de comunicación o sincronización entre ellas.

Ejemplo: Multiplicación de Matrices. La multiplicación de dos matrices es un ejemplo clásico de un problema de paralelismo de datos. Para calcular un elemento en la matriz resultante, se realiza una operación que implica solo una fila de la primera matriz y una columna de la segunda matriz. Estas operaciones son independientes unas de otras y pueden realizarse en paralelo para todos los elementos de la matriz resultante.

Parámetros de entrada: Dos matrices, A y B.
Operación: Para cada elemento C[i][j]
C[i][j] en la matriz resultante C, calcula la suma de los productos de los elementos correspondientes en la fila i de la matriz A y la columna j de la matriz B.
Salida: Una nueva matriz C que es el producto de A y B.
Problema de Paralelismo de Tareas
En un problema de paralelismo de tareas, diferentes tareas (o procesos), que pueden ser diferentes operaciones, se ejecutan en paralelo. Cada tarea puede trabajar en una parte diferente del problema o en un problema completamente diferente, y pueden requerir comunicación o sincronización entre ellas.

Ejemplo: Procesamiento de Transacciones en un Sistema Bancario. Considera un sistema bancario en línea donde las transacciones de los clientes se procesan en paralelo. Cada transacción (retiro, depósito, transferencia, etc.) es una tarea independiente y puede requerir acceso a diferentes cuentas o servicios. Aunque las transacciones son independientes, pueden requerir sincronización para garantizar la coherencia de los datos (por ejemplo, asegurando que los balances de las cuentas se actualicen de manera atómica).

Parámetros de entrada: Un conjunto de transacciones bancarias, cada una con su tipo y los datos necesarios para su procesamiento.
Operaciones: Cada transacción se procesa de acuerdo a su tipo; por ejemplo, actualizar el balance de una cuenta, transferir fondos entre cuentas, etc.
Salida: Actualización de los balances de las cuentas y registros de las transacciones procesadas.

Data-parallel problems are ideal candidates for the GPU. The reason is because the
GPU architecture works best when all threads execute the same instructions but on different data. On the other hand, task-parallel problems are best suited for the CPU. The reason is because the CPU architecture allows different tasks to be executed on each thread.
This classification scheme is critical for achieving the best partition of the problem domain, which is in fact the first step when designing a parallel algorithm. It also provides
useful information when choosing the best hardware for the implementation (CPU or
GPU). Computational physics problems often classify as data-parallel, therefore they are
good candidates for a massive parallelization on GPU. Since the aim of this work is to
provide a survey on parallel computing for computational physics, most of the explanations will be in the context of data-parallel problems.

################### MEDIDAS DE RENDIMIENTO #####################
Set de métricas para cuantificar la calidad de un algoritmo. Para algortmos secuenciales tiempo y espacio eran suficientes. En caso de algoritmos paralelos el speedup y la eficiencia también son necesarias para estudiar su calidad y la mejoría. En el lado experimental además se tienen medidas como el ancho de banda de memoria y los FLOPS (floating point operations per second).

Dado un problema de tamaño n, el tiempo de ejecución de un algoritmo paralelo usando p procesadores se denomina:
T(n,p)

El work y el span son la base para las métricas de eficiencia y speedup. Ambas mediciones son importantes porque dan límites a la computación en paralelo.
work: T(n,1) es el tiempo total necesitado para ejecutar un algoritmo paralelo usando solo un procesador, denominado T(n,1). Es decir, el tiempo que se demora el algortimo secuencial.

span: T(n,00)la mayor cantidad de tiempo que se necesita para ejecutar el algoritmo usando infinitos procesadores

Ley del trabajo T(n,p)>= T(n,1)/p : se demorará por lo menos 1/8 del trabajo. Algoritmos paralelos corren mejor cuando el trabajo por procesador está balanceado

Ley del span T(n,p) >= T(n, 00) un algoritmo paralelo no se demorará menos de lo que se demora la mínima cantidad de tiempo en que un procesador termine su trabajo en una máquina de ifninitos procesadores.

El speedup: mide cuán rápido es el algoritmo paralelo vs el mejor algoritmo secuencial con una cantidad de procesadores p S_p = T_s(n,1)/T(n,p)



Work y Span son conceptos fundamentales en la teoría de la computación paralela para analizar el rendimiento de algoritmos paralelos.

Work (W): Representa el total de tiempo o cantidad de esfuerzo computacional requerido para completar una tarea si se ejecutara en un solo procesador. Es la suma de todos los pasos de computación realizados por el algoritmo. En otras palabras, es el tiempo de ejecución del algoritmo en un entorno secuencial sin paralelismo.

Span (S), también conocido como profundidad crítica, es el tiempo que toma ejecutar el algoritmo en un número infinito de procesadores, es decir, el tiempo que toma completar la tarea más larga desde el inicio hasta el fin, considerando que todas las tareas paralelas se ejecutan instantáneamente. Representa el camino más largo de dependencias en la ejecución del algoritmo.

Utilidad: Estos conceptos se utilizan para calcular la eficiencia, el speedup y la escalabilidad de algoritmos paralelos. Ayudan a identificar cuánto puede mejorarse el rendimiento mediante paralelización y cuál es el límite teórico de esta mejora. Work y Span permiten calcular el speedup teórico máximo usando la Ley de Brent, que afirma que el tiempo de ejecución T_p con p procesadores es al menos 
W/p+S, dando una medida de cuán bien un algoritmo puede ser paralelizado.

Diferencia/Relación entre 
T(n,1) y T_s(n,1)

T(n,1) representa el tiempo que toma ejecutar un algoritmo paralelo en un solo procesador, es decir, sin aprovechar el paralelismo. Este tiempo incluye todo el overhead introducido por el diseño paralelo del algoritmo, incluso cuando se ejecuta secuencialmente.

T_s(n,1), por otro lado, es el tiempo de ejecución del mejor algoritmo secuencial conocido para el mismo problema. Este tiempo es típicamente el más optimizado posible para una ejecución en un solo procesador, sin considerar estructuras de paralelismo.

Diferencia/Relación: La principal diferencia entre 
T(n,1) y T_s(n,1) radica en que T(n,1) puede ser mayor que T_s(n,1) debido al overhead adicional de paralelización que se presenta incluso en la ejecución en un solo núcleo. T_s(n,1) es una medida de eficiencia para el algoritmo secuencial óptimo, mientras que T(n,1) refleja la eficiencia de un algoritmo paralelo cuando se ve forzado a operar en un entorno secuencial.

Modelos de Speedup y sus Características Principales
Los tres modelos de speedup (escalado fijo, escalado por tamaño, y tiempo fijo) reflejan diferentes maneras de entender y medir cómo el paralelismo afecta el rendimiento.

Fixed-size speedup (Escalado fijo): Evalúa cómo cambia el tiempo de ejecución cuando se aumenta el número de procesadores mientras se mantiene fijo el tamaño del problema. Su característica principal es medir directamente el efecto del paralelismo en la velocidad de ejecución para un problema de tamaño constante.

Scaled speedup (Escalado por tamaño): Aumenta el tamaño del problema proporcionalmente al número de procesadores. Esto refleja escenarios donde la cantidad de trabajo crece con los recursos disponibles, manteniendo constante la carga de trabajo por procesador. Su característica principal es medir la capacidad de un sistema para manejar eficientemente problemas más grandes a medida que se dispone de más procesadores.

Fixed-time speedup (Tiempo fijo): Consiste en ajustar el tamaño del problema de manera que el tiempo de ejecución se mantenga constante mientras se aumenta el número de procesadores. Este modelo es menos común pero útil para evaluar cómo diferentes configuraciones de procesadores pueden resolver problemas de mayor tamaño en el mismo tiempo.


El "linear speedup" se refiere al ideal en la computación paralela donde duplicar el número de procesadores (o unidades de procesamiento) resulta en una reducción a la mitad del tiempo de ejecución del programa. Es decir, si un programa se ejecuta en un tiempo 
 
T usando 
 
N procesadores, el objetivo sería que se ejecute en un tiempo 
 
/
2
T/2 usando 
2
 
2N procesadores, logrando así un speedup lineal.

¿Por qué se cree que es alcanzable?
Independencia de tareas: En problemas donde las tareas son completamente independientes y no requieren comunicación entre ellas, teóricamente es posible asignar una tarea por procesador, resultando en un speedup lineal. Esto se aplica bien a problemas "embarazosamente paralelos".

Eficiencia de la paralelización: Para ciertos problemas y algoritmos, especialmente aquellos diseñados con el paralelismo en mente desde el principio, se pueden minimizar los cuellos de botella y la necesidad de sincronización o comunicación, acercándose al speedup lineal.

Uso óptimo de recursos: Con una gestión eficaz de los recursos de hardware y una planificación de tareas que minimice el tiempo de inactividad de los procesadores, algunos creen que el speedup lineal es un objetivo alcanzable.

¿Por qué se cree que no es alcanzable?
Ley de Amdahl: La ley de Amdahl establece que el speedup de un programa está limitado por la porción del programa que debe ejecutarse secuencialmente. Incluso una pequeña parte del código que no se puede paralelizar puede limitar significativamente el speedup total alcanzable, haciendo que el speedup lineal sea prácticamente imposible para muchos programas.

Sobrecostos de paralelización: La paralelización introduce sobrecostos adicionales, como la comunicación entre procesadores, sincronización, y el manejo de datos compartidos. Estos sobrecostos pueden aumentar con el número de procesadores, reduciendo el speedup efectivo.

Problemas de escalabilidad: A medida que aumenta el número de procesadores, la gestión eficiente de la comunicación y la distribución de tareas se vuelve más compleja. Problemas como el desbalance de carga, donde algunos procesadores están inactivos mientras otros están sobrecargados, pueden impedir el speedup lineal.

Limitaciones de hardware y software: Las arquitecturas de hardware y los sistemas operativos tienen limitaciones prácticas en cuanto al paralelismo que pueden gestionar eficazmente. Los cuellos de botella en el acceso a memoria, la latencia de la red en sistemas distribuidos, y la coherencia de caché son factores que pueden limitar el speedup.

Conclusión
El speedup lineal es un ideal hacia el cual se esfuerzan los diseñadores de sistemas paralelos, pero en la práctica, es difícil de alcanzar para muchos problemas debido a las limitaciones inherentes de la computación paralela y la ley de Amdahl. Sin embargo, para problemas específicos altamente paralelizables y con una adecuada optimización, se puede aproximar a este ideal. La búsqueda del speedup lineal impulsa la innovación en el diseño de algoritmos paralelos y la arquitectura de sistemas computacionales.

Speedup medido con la Ley de Amdahl
La Ley de Amdahl se utiliza para predecir el speedup teórico máximo de un proceso al paralelizarlo, enfocándose en la porción del programa que no se puede paralelizar. La fórmula básica es:

S(p)=  1/ [(1−P)+ P/p]
donde:

S(p) es el speedup teórico máximo al usar 
p procesadores.

P es la proporción del programa que se puede paralelizar.

1−P es la proporción que debe ejecutarse secuencialmente.

Ejemplo con la Ley de Amdahl:

Problema: Consideremos el caso de un algoritmo de ordenamiento que necesita ordenar una gran cantidad de datos. Supongamos que el 95% del algoritmo se puede paralelizar (por ejemplo, usando ordenamiento por mezcla paralelo), mientras que el 5% restante (como la combinación final de subconjuntos ordenados) debe ejecutarse secuencialmente.

Si aplicamos la Ley de Amdahl para calcular el speedup teórico al usar 16 procesadores (p=16), tenemos:

S(16) = 1 / [(1−0.95)+0.95/16] ≈ 5.263

Esto significa que, bajo las condiciones ideales descritas por la Ley de Amdahl, podríamos esperar un speedup máximo de aproximadamente 5.26 veces más rápido que la versión secuencial, sin importar cuántos procesadores adicionales utilicemos.

Speedup medido con la Ley de Gustafson
La Ley de Gustafson propone una perspectiva diferente, sugiriendo que el tamaño del problema puede escalar con el número de procesadores, ofreciendo un enfoque más realista para muchas aplicaciones prácticas. La fórmula de Gustafson es:

S′(p)=p−α(p−1)

donde:

S′(p) es el speedup teórico según Gustafson al usar p procesadores.

α es la fracción del tiempo que se dedica a las secciones secuenciales del algoritmo.

p es el número de procesadores.

Ejemplo con la Ley de Gustafson:

Problema: Supongamos que tenemos una aplicación de simulación de clima que puede adaptarse para utilizar más recursos computacionales efectivamente al aumentar el número de procesadores, permitiéndonos hacer simulaciones más detalladas o cubrir áreas geográficas más amplias.

Si consideramos que el 10% del tiempo de ejecución es inherente a tareas secuenciales (α=0.10) y decidimos ejecutar nuestra simulación usando 100 procesadores (p=100), aplicando la Ley de Gustafson obtenemos:

S′(100) = 100 − 0.10*(100 − 1) = 100 − 9.9 = 90.1

Esto sugiere que, ajustando el tamaño del problema al número de procesadores disponibles, podemos esperar que la aplicación se ejecute hasta 90.1 veces más rápido que en un solo procesador, según la Ley de Gustafson.

Ambos ejemplos ilustran cómo las leyes de Amdahl y Gustafson ofrecen perspectivas valiosas sobre el speedup teórico en la computación paralela, cada una desde un ángulo diferente, reflejando la importancia de considerar tanto la capacidad de paralelización del problema como la escalabilidad del tamaño del problema con respecto a los recursos computacionales disponibles.


Las arquitecturas de GPU (Unidad de Procesamiento Gráfico) están diseñadas para procesar grandes cantidades de datos en paralelo, lo que las hace excepcionalmente buenas para gráficos y cálculos científicos que pueden ser paralelizados. Sin embargo, maximizar el rendimiento de la memoria en una GPU puede ser más desafiante que en una CPU (Unidad de Procesamiento Central) por varias razones:

Dependencia del Problema
El rendimiento de la memoria en las GPUs es altamente dependiente del problema específico y de cómo se organizan y acceden los datos. Para obtener el máximo rendimiento, es crucial que las estructuras de datos estén alineadas con las características de la memoria de la GPU, permitiendo que se lean o escriban grandes bloques de datos simultáneamente.

Accesos a Memoria
Los accesos irregulares a la memoria, donde los patrones de lectura y escritura no siguen una secuencia predecible, pueden disminuir significativamente el rendimiento. Esto se debe a que las GPUs prefieren patrones de acceso a memoria coalescidos, donde múltiples hilos de ejecución leen o escriben en direcciones de memoria contiguas en un solo ciclo de reloj.

Alineación de Datos
La alineación de datos se refiere a la organización de los datos en memoria de manera que coincidan con los límites naturales de la misma. En GPUs, esto puede significar asegurarse de que las estructuras de datos comiencen en direcciones que sean múltiplos de ciertos tamaños (por ejemplo, 32 bytes), lo que facilita el acceso coalescido a la memoria.

Tamaño de los Datos
Los tamaños de los bloques de datos que se procesan también juegan un papel importante. Bloques de tamaño inadecuado pueden llevar a un desperdicio de recursos de memoria y a una disminución en el rendimiento debido a la necesidad de realizar más accesos a memoria de lo necesario.

Caché L2 en Arquitecturas Fermi y Kepler
Las arquitecturas Fermi y Kepler de Nvidia introdujeron mejoras significativas en la gestión de la memoria, incluyendo una caché L2 para la memoria global. La caché L2 ayuda a mitigar los problemas de accesos irregulares a la memoria y la desalineación de datos, actuando como un buffer que puede almacenar datos temporalmente y servir múltiples accesos a datos cercanos sin requerir accesos adicionales a la memoria global, que es más lenta.

Arquitecturas de GPU
Las arquitecturas de GPU, como Fermi, Kepler, y sus sucesoras Maxwell, Pascal, Volta, Turing, y Ampere, han sido diseñadas por Nvidia, cada una con mejoras en el rendimiento, eficiencia energética, y capacidades de procesamiento en paralelo. Estas arquitecturas han evolucionado para soportar no solo gráficos computacionales sino también cálculos científicos y de inteligencia artificial, ofreciendo una amplia gama de aplicaciones. Cada nueva generación ha traído mejoras en la jerarquía de memoria, la cantidad de núcleos de procesamiento, y características específicas como unidades de tensor para cálculos de IA, todos diseñados para mejorar el rendimiento y la eficiencia en una amplia gama de cargas de trabajo.



############### MODELOS DE COMPUTACION #################
El modelo PRAM (Parallel Random Access Machine) es un modelo teórico utilizado para el diseño y análisis de algoritmos paralelos. Tiene varias variantes que se diferencian en cómo manejan el acceso concurrente a la memoria:

EREW (Exclusive Read Exclusive Write): En esta variante, ningún procesador puede leer o escribir simultáneamente en la misma ubicación de memoria. Garantiza la exclusividad tanto en lecturas como en escrituras.

CREW (Concurrent Read Exclusive Write): Permite que múltiples procesadores lean de la misma ubicación de memoria simultáneamente, pero solo un procesador puede escribir en una ubicación de memoria en un momento dado.

ERCW (Exclusive Read Concurrent Write): Los procesadores pueden leer exclusivamente, pero varios procesadores pueden escribir en la misma ubicación de memoria al mismo tiempo. Esta variante es menos común debido a los desafíos que presenta la escritura concurrente.

CRCW (Concurrent Read Concurrent Write): Permite lecturas y escrituras concurrentes en la misma ubicación de memoria. Esta es la variante más poderosa pero también la más compleja de manejar debido a los conflictos potenciales de escritura.

Ejemplos para Variantes del Modelo PRAM
CREW - Búsqueda Binaria Paralela: Un buen ejemplo de uso para el modelo CREW es la búsqueda binaria en un arreglo ordenado. Dado que la búsqueda binaria divide el espacio de búsqueda a la mitad en cada paso, se puede asignar a cada procesador un segmento único del arreglo para leer y determinar en qué segmento se debe continuar la búsqueda en el siguiente paso. Aquí, múltiples procesadores leen de diferentes partes del arreglo (lectura concurrente), pero solo uno actualizará el índice de búsqueda (escritura exclusiva).

Entrada: Un arreglo ordenado A de tamaño n y un valor objetivo x.
Proceso: Divide el espacio de búsqueda entre todos los procesadores disponibles. Cada procesador lee su segmento asignado para determinar si contiene el valor objetivo. Se actualiza el rango de búsqueda basado en los resultados de todos los procesadores.
Salida: La posición de x en A o una indicación de que x no está presente.
CRCW - Suma de Prefijos: La suma de prefijos es un algoritmo que puede implementarse eficientemente bajo el modelo CRCW. El objetivo es calcular un nuevo arreglo donde cada elemento en la posición i es la suma de todos los elementos desde la posición 0 hasta i en el arreglo original. En el modelo CRCW, múltiples procesadores pueden trabajar en calcular sumas parciales simultáneamente y escribir sus resultados en el mismo momento, utilizando técnicas como la suma en árbol paralelo.

Entrada: Un arreglo A de n elementos.
Proceso: Cada procesador calcula sumas parciales basadas en la posición y los resultados de otros procesadores, aprovechando el acceso concurrente para lectura y escritura, facilitando la combinación de resultados.
Salida: Un nuevo arreglo donde cada posición i contiene la suma de los elementos de A[0] a A[i].
Estos ejemplos ilustran cómo las distintas variantes del modelo PRAM pueden ser utilizadas para implementar algoritmos paralelos de manera eficiente, tomando en cuenta las restricciones específicas de acceso a la memoria de cada variante.

############ modelos de computación paralela ##########
. PRAM (Parallel Random Access Machine)
Descripción: El modelo PRAM asume un número ilimitado de procesadores con acceso instantáneo a una memoria compartida común. Se distingue por cómo maneja los accesos concurrentes a la misma ubicación de memoria, con variantes como EREW, CREW, ERCW y CRCW.

Ejemplo de Problema: Algoritmo de Suma de Prefijos. Dado un arreglo de números, el objetivo es calcular un nuevo arreglo donde cada elemento en la posición 
 
i es la suma de todos los elementos desde la posición 
0
0 hasta 
 
i en el arreglo original. En el modelo CRCW PRAM, múltiples procesadores pueden simultáneamente leer de la misma ubicación de memoria y escribir en distintas ubicaciones, permitiendo que el cálculo de cada elemento del arreglo de salida se realice en paralelo.

1. EREW (Exclusive Read Exclusive Write)
En este modelo, ningún procesador puede leer o escribir simultáneamente en la misma ubicación de memoria.

Ejemplo: Ordenamiento por Mezcla (Merge Sort). Cada procesador se encarga de ordenar una porción del arreglo de forma independiente. Las lecturas y escrituras son exclusivas para evitar conflictos, y luego se mezclan las subsecuencias ordenadas de manera jerárquica. La exclusividad garantiza que no habrá conflictos de acceso a la memoria durante la combinación.

2. CREW (Concurrent Read Exclusive Write)
Permite que varios procesadores lean de la misma ubicación de memoria simultáneamente, pero solo un procesador a la vez puede escribir en cualquier ubicación de memoria.

Ejemplo: Búsqueda Binaria en un Arreglo Ordenado. Múltiples procesadores pueden leer el mismo elemento del arreglo para compararlo con el valor objetivo al mismo tiempo, pero solo un procesador escribirá el resultado de la búsqueda (por ejemplo, la posición del valor objetivo) en una ubicación de memoria dedicada.

3. ERCW (Exclusive Read Concurrent Write)
Los procesadores pueden leer de ubicaciones de memoria exclusivas, pero múltiples procesadores pueden escribir en la misma ubicación de memoria al mismo tiempo.

Ejemplo: Conteo de Ocurrencias de Elementos. Cada procesador lee diferentes partes de un arreglo para buscar un elemento específico. Todos los procesadores pueden incrementar un contador global de forma concurrente cada vez que encuentran el elemento. Este modelo necesita mecanismos para resolver conflictos de escritura concurrente, como utilizar operaciones atómicas.

4. CRCW (Concurrent Read Concurrent Write)
Permite que múltiples procesadores lean y escriban en la misma ubicación de memoria al mismo tiempo.

Ejemplo: Cálculo del Mínimo o Máximo en un Arreglo. Todos los procesadores leen simultáneamente todos los elementos del arreglo y compiten para escribir el valor mínimo (o máximo) encontrado en una ubicación de memoria compartida. Para manejar las escrituras concurrentes, se puede usar una política como "priority" donde el procesador con el identificador más bajo gana en caso de conflicto o "common" donde todos los procesadores intentan escribir el mismo valor.

Cada una de estas variantes del modelo PRAM facilita diferentes tipos de paralelismo y sincronización, adaptándose a los requisitos específicos de accesibilidad y actualización de la memoria en algoritmos paralelos.

2. PMH (Parallel Memory Hierarchy)
Descripción: PMH considera la jerarquía de memoria en el diseño de algoritmos paralelos, reflejando la estructura real de computadoras modernas. Enfatiza en cómo la disposición de la memoria afecta el rendimiento del algoritmo paralelo.

Ejemplo de Problema: Multiplicación de Matrices utilizando bloques. Este problema se beneficia de la jerarquía de memoria al dividir las matrices en bloques que se ajustan en los niveles más rápidos de la memoria (como la caché), reduciendo el número de accesos a la memoria principal. La idea es cargar bloques de las matrices en la caché, realizar el cálculo necesario, y luego proceder al siguiente bloque.

3. BSP (Bulk Synchronous Parallel)
Descripción: El modelo BSP organiza el cálculo en superpasos, cada uno consistiendo en un período de cálculo local, una fase de comunicación y una barrera de sincronización. Es útil para diseñar algoritmos en arquitecturas distribuidas, equilibrando carga y comunicación.

Ejemplo de Problema: Resolución de Sistemas Lineales distribuidos. Se puede usar BSP para distribuir las filas de la matriz del sistema lineal entre diferentes procesadores. Cada procesador realiza operaciones en sus filas durante la fase de cálculo, luego durante la fase de comunicación, los procesadores pueden intercambiar resultados necesarios para las operaciones de las siguientes etapas, como en métodos iterativos (Jacobi, Gauss-Seidel).

4. LogP
Descripción: LogP se centra en la latencia (L), el overhead de procesamiento (o), el gap (g) que es el tiempo mínimo entre envíos de mensajes, y el número de procesadores (P), ofreciendo una manera de considerar los costos de comunicación y sincronización en algoritmos distribuidos.

Ejemplo de Problema: Ordenamiento Paralelo. En un escenario de ordenamiento distribuido, donde diferentes segmentos de un gran arreglo se asignan a distintos nodos en una red, LogP puede guiar el diseño del algoritmo para minimizar el tiempo de comunicación y el overhead al coordinar la fusión de segmentos ordenados localmente en un resultado global ordenado. Se presta especial atención a minimizar la latencia y el overhead de la comunicación entre nodos.

Cada uno de estos modelos ofrece una perspectiva única para enfrentar y optimizar diferentes aspectos de los cálculos paralelos y distribuidos, desde la concurrencia y la jerarquía de memoria hasta la comunicación y la sincronización.


############# Memoria compartida ###################3
Claro, vamos a profundizar en la sección 5.1, que se refiere al **modelo de programación de memoria compartida**.

En el modelo de programación de **memoria compartida**, múltiples hilos (o procesos) ejecutan en paralelo y tienen acceso a una memoria común. Este modelo es típicamente utilizado en sistemas multiprocesador o multihilo donde los procesadores pueden ser núcleos dentro de un mismo chip. La comunicación entre hilos se realiza accediendo a variables compartidas almacenadas en esta memoria común.

La principal ventaja de este modelo es su simplicidad conceptual para el programador: no es necesario manejar explícitamente el envío y recepción de mensajes entre procesos. Sin embargo, esto también introduce desafíos relacionados con la sincronización y la coherencia de datos, ya que los accesos concurrentes a la misma ubicación de memoria deben ser manejados cuidadosamente para evitar condiciones de carrera y asegurar la integridad de los datos.

#### Ejemplo: Uso de OpenMP en Suma de Elementos de un Arreglo

**OpenMP** (Open Multi-Processing) es una API de programación en C, C++ y Fortran que soporta el paralelismo de memoria compartida en plataformas incluyendo Unix y Windows. Proporciona una manera simple y flexible de escribir programas paralelos para plataformas con memoria compartida.

Supongamos que queremos sumar los elementos de un arreglo grande. En un enfoque secuencial, simplemente iteraríamos sobre el arreglo y sumaríamos cada elemento a un acumulador. Sin embargo, este proceso se puede paralelizar eficientemente usando OpenMP.

**Código Secuencial**:
```c
int sum = 0;
for(int i = 0; i < N; ++i) {
    sum += array[i];
}
```

**Código Paralelizado con OpenMP**:
```c
int sum = 0;
#pragma omp parallel for reduction(+:sum)
for(int i = 0; i < N; ++i) {
    sum += array[i];
}
```

En el ejemplo paralelizado:

- La directiva `#pragma omp parallel for` le indica al compilador que el bucle for debe ser ejecutado en paralelo por los hilos disponibles. 

- La cláusula `reduction(+:sum)` es crucial: especifica que la variable `sum` se utilizará para recolectar un resultado final a partir de la suma parcial calculada por cada hilo. OpenMP se encarga de realizar la suma de estas sumas parciales de manera segura, evitando condiciones de carrera y asegurando que cada hilo tenga su propia copia temporal de la variable `sum` durante el bucle.

Este ejemplo ilustra cómo el modelo de memoria compartida puede simplificar la paralelización de tareas que, de otro modo, requerirían una gestión explícita de la división del trabajo y la recolección de resultados entre múltiples hilos o procesos. La facilidad de uso de OpenMP lo hace especialmente atractivo para paralelizar rápidamente aplicaciones existentes con mínimas modificaciones en el código.

#################### 5.2 Modelo de Paso de Mensajes ############

Por supuesto, vamos a profundizar en las secciones 5.2, 5.3 y 5.4, que tratan sobre los modelos de programación de paso de mensajes, paralelismo implícito y esqueletos algorítmicos, respectivamente.


En el **modelo de paso de mensajes**, los procesadores o nodos de computación tienen su propia memoria local y la comunicación entre ellos se realiza explícitamente mediante el envío y recepción de mensajes. Este modelo es ampliamente utilizado en sistemas distribuidos y computación en clústeres.

**Ejemplo: Cálculo Distribuido de Pi usando MPI**

**MPI** (Message Passing Interface) es una biblioteca estándar para el paso de mensajes en programación paralela que se utiliza ampliamente en sistemas de cómputo de alto rendimiento.

Para ilustrar este modelo, consideremos el problema de calcular una aproximación de Pi usando el método de Monte Carlo. Este método implica generar puntos aleatorios dentro de un cuadrado que inscribe un cuarto de círculo y determinar qué fracción de esos puntos cae dentro del cuarto de círculo.

**Código con MPI**:
```c
#include <mpi.h>
#include <stdlib.h>
#include <time.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    
    int local_count = 0, total_count;
    int num_local_trials = 10000;
    
    srand(time(NULL) + world_rank);
    for(int i = 0; i < num_local_trials; ++i) {
        double x = (double)rand() / RAND_MAX;
        double y = (double)rand() / RAND_MAX;
        if(x * x + y * y <= 1) local_count++;
    }
    
    MPI_Reduce(&local_count, &total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
    
    if(world_rank == 0) {
        printf("Pi Approximation: %f\n", 4 * total_count / (double)(num_local_trials * world_size));
    }
    
    MPI_Finalize();
    return 0;
}
```
Cada proceso calcula una porción de los puntos y luego se utiliza `MPI_Reduce` para sumar los conteos de todos los procesos y calcular Pi en el proceso maestro.

############## 5.3 Paralelismo Implícito #################

El **paralelismo implícito** elimina la necesidad de que los programadores especifiquen cómo se debe paralelizar el programa. En su lugar, el compilador o el entorno de ejecución determinan cómo distribuir el trabajo entre los procesadores disponibles.

**Ejemplo: Programación Funcional en Haskell**

En Haskell, la evaluación perezosa y el modelo de ejecución funcional permiten que el compilador y el entorno de tiempo de ejecución identifiquen oportunidades de paralelismo sin intervención explícita del programador.

```haskell
import Control.Parallel.Strategies

-- Definir una función que usa evaluación paralela
parMapReduce :: (a -> b) -> ([b] -> c) -> [a] -> c
parMapReduce mapFunc reduceFunc inputList =
  reduceFunc (map mapFunc inputList `using` parList rseq)
```
Este fragmento de código demuestra cómo una combinación de `map` y `reduce` puede ejecutarse en paralelo utilizando estrategias de paralelismo implícitas.

################# 5.4 Esqueletos Algorítmicos ####################3

Los **esqueletos algorítmicos** (también conocidos como patrones de diseño paralelo) son abstracciones de alto nivel que encapsulan patrones comunes de paralelismo. Los programadores utilizan estos esqueletos para implementar algoritmos paralelos sin tener que lidiar con los detalles de bajo nivel de la paralelización.

**Ejemplo: Paralelismo de Mapa utilizando SkeTo en C++**

SkeTo es una biblioteca que proporciona esqueletos algorítmicos para C++. Un ejemplo clásico es el uso de un esqueleto de `map` para aplicar una función a cada elemento de una colección en paralelo.

```c++
#include <vector>
#include <sketo/Map.h>

int square(int x) { return x *

Claro, exploraremos cuatro tipos de esqueletos algorítmicos comunes: **Map**, **Reduce**, **Pipeline**, y **Farm**. Para cada uno, proporcionaré un ejemplo conceptual y explicaré cómo se puede aplicar en un contexto de programación paralela.

### 1. Map (Transformación)

El esqueleto **Map** aplica una función a cada elemento de una colección de datos de manera independiente, produciendo una nueva colección con los resultados.

**Ejemplo**: **Paralelización de la Transformación de una Imagen**

Supongamos que queremos convertir una imagen a escala de grises. La operación se aplica de manera independiente a cada píxel de la imagen:

```python
# Función que convierte un píxel RGB a escala de grises
def to_grayscale(pixel):
    r, g, b = pixel
    return 0.3 * r + 0.59 * g + 0.11 * b

# Imagen representada como una lista de píxeles RGB
image = [...]
# Aplicar 'to_grayscale' a cada píxel de la imagen
grayscale_image = map(to_grayscale, image)
```

### 2. Reduce (Reducción)

El esqueleto **Reduce** combina todos los elementos de una colección utilizando una operación binaria para reducirlos a un único valor.

**Ejemplo**: **Suma de Elementos de un Arreglo**

Para calcular la suma de todos los elementos en un arreglo de manera paralela, se pueden sumar subconjuntos del arreglo en paralelo y luego sumar los resultados intermedios.

```python
from functools import reduce

# Función para sumar dos elementos
def add(x, y):
    return x + y

# Arreglo de números
numbers = [1, 2, 3, 4, 5]
# Calcular la suma total de los elementos del arreglo
total_sum = reduce(add, numbers)
```

### 3. Pipeline (Tuberías)

El esqueleto **Pipeline** organiza las tareas en una secuencia de etapas, donde la salida de una etapa es la entrada de la siguiente. Cada etapa puede procesar su entrada de manera concurrente.

**Ejemplo**: **Procesamiento de Solicitudes Web**

Una solicitud web puede procesarse en varias etapas: parseo, autenticación, procesamiento del negocio y generación de la respuesta.

```python
def parse_request(request):
    # Parsear la solicitud
    return parsed_request

def authenticate(request):
    # Autenticar la solicitud
    return authenticated_request

def process_request(request):
    # Procesar la solicitud
    return response

# Cada función representa una etapa en el pipeline
```

### 4. Farm (Granja de Tareas)

El esqueleto **Farm** distribuye un flujo de tareas independientes entre múltiples trabajadores para su procesamiento paralelo, a menudo utilizado para equilibrar la carga.

**Ejemplo**: **Renderizado Paralelo de Imágenes 3D**

En un sistema de renderizado, cada imagen a ser renderizada representa una tarea independiente que puede ser distribuida entre diferentes nodos de renderizado.

```python
# Lista de tareas de renderizado, donde cada tarea es una descripción de una imagen
render_tasks = [...]

# Función que renderiza una imagen
def render(task):
    # Renderizar la imagen
    return image

# Distribuir las tareas de renderizado entre diferentes trabajadores/nodos
rendered_images = [render(task) for task in render_tasks]
```

Estos ejemplos ilustran cómo los esqueletos algorítmicos pueden abstraer y simplificar el diseño e implementación de algoritmos paralelos, permitiendo a los desarrolladores concentrarse en la lógica de alto nivel del problema sin preocuparse por los detalles específicos del paralelismo.

Para realizar cada función del procesamiento de solicitudes web en paralelo usando un modelo de **pipeline** (tubería), es importante comprender que, aunque las etapas se procesan secuencialmente para una única solicitud, el modelo de pipeline permite que diferentes solicitudes o partes de solicitudes sean procesadas en paralelo en diferentes etapas del pipeline. Es decir, mientras una solicitud está siendo autenticada, otra puede estar siendo parseada, y una tercera puede estar en el proceso de generación de respuesta, todas al mismo tiempo. Aquí se muestra cómo se podría implementar esto en un entorno de programación paralela o concurrente:

### Ejemplo Detallado: Procesamiento de Solicitudes Web en Paralelo

Imagina que tienes un servidor web que necesita manejar tres etapas principales para cada solicitud: **parseo**, **autenticación** y **procesamiento**. En un modelo de pipeline paralelo, tan pronto como una solicitud completa una etapa, se mueve a la siguiente, mientras que otras solicitudes pueden estar pasando por etapas anteriores o posteriores.

1. **Parseo de la Solicitud**: Convierte la solicitud cruda entrante en una estructura de datos manejable. Mientras una solicitud está siendo parseada, otras solicitudes pueden estar esperando en una cola o siendo procesadas en etapas posteriores si ya han sido parseadas.

2. **Autenticación**: Verifica si la solicitud proviene de un usuario autenticado y tiene permisos para realizar la acción solicitada. Solicitudes diferentes pueden ser autenticadas simultáneamente por distintos hilos o procesos.

3. **Procesamiento de la Solicitud**: Realiza la lógica de negocio requerida por la solicitud, como acceder a una base de datos o realizar cálculos. Al igual que en las etapas anteriores, múltiples solicitudes pueden estar siendo procesadas al mismo tiempo, cada una en distintos puntos de la tubería.

Para implementar este pipeline en un entorno de software que soporte concurrencia o paralelismo, como Node.js (que usa un modelo de eventos no bloqueantes) o utilizando frameworks y bibliotecas en Python (como asyncio o concurrent.futures), se crearían tareas o procesos separados para cada etapa del pipeline. Aquí un pseudo-código usando Python con asyncio para ilustrar el concepto:

```python
import asyncio

async def parse_request(request):
    # Simula el parseo de la solicitud
    await asyncio.sleep(1)
    return "Parsed Request"

async def authenticate(request):
    # Simula la autenticación
    await asyncio.sleep(1)
    return "Authenticated Request"

async def process_request(request):
    # Simula el procesamiento
    await asyncio.sleep(1)
    return "Response"

async def handle_request(request):
    parsed = await parse_request(request)
    authenticated = await authenticate(parsed)
    response = await process_request(authenticated)
    return response

# Manejar múltiples solicitudes
async def main():
    requests = ["request1", "request2", "request3"]
    tasks = [handle_request(request) for request in requests]
    responses = await asyncio.gather(*tasks)
    print(responses)

asyncio.run(main())
```

En este ejemplo, cada función (`parse_request`, `authenticate`, `process_request`) puede ejecutarse en paralelo para diferentes solicitudes gracias al modelo de concurrencia de `asyncio`. Mientras una solicitud está en la etapa de autenticación, otra puede estar siendo parseada, y otra puede estar en procesamiento, maximizando así la utilización de los recursos y reduciendo el tiempo de respuesta total.


############ arquitecturas de computadores #################3
La parte 6 del documento trata sobre las **arquitecturas de computadoras** utilizadas en la computación paralela, profundizando en cómo el diseño de la arquitectura influye en el rendimiento de los algoritmos paralelos. Veamos cada sección con ejemplos, especialmente enfocándonos en las arquitecturas UMA y NUMA.

### 6.1 Flynn's Taxonomy

Flynn's Taxonomy clasifica las arquitecturas de computadoras en cuatro categorías basadas en el número de flujos de instrucciones y datos que pueden procesar simultáneamente:

- **SISD (Single Instruction, Single Data)**: Tradicional CPU secuencial. Ejemplo: procesadores antiguos como el Intel 8086.
- **SIMD (Single Instruction, Multiple Data)**: Un procesador ejecuta la misma instrucción en múltiples datos simultáneamente. Ejemplo: modernas GPUs utilizadas para procesamiento gráfico o computación científica.
- **MISD (Multiple Instruction, Single Data)**: Múltiples instrucciones operan en el mismo conjunto de datos. Este es más teórico y raro en la práctica.
- **MIMD (Multiple Instruction, Multiple Data)**: Múltiples procesadores ejecutan diferentes instrucciones en diferentes conjuntos de datos. Ejemplo: Sistemas multiprocesador modernos y clústeres de computadoras.

### 6.2 Memory Architectures and Organizations

Las arquitecturas de memoria se pueden clasificar en **UMA** y **NUMA**, que se refieren a cómo se organiza y accede a la memoria en sistemas con múltiples procesadores.

#### UMA (Uniform Memory Access)

En UMA, todos los procesadores comparten una memoria común con igual tiempo de acceso, independientemente del procesador que realice la solicitud. Es común en sistemas con pocos procesadores o en aquellos diseñados para minimizar las diferencias en el tiempo de acceso a la memoria.

**Ejemplo UMA**: Considera una computadora con un procesador de cuatro núcleos donde cada núcleo puede acceder a la memoria RAM con la misma latencia. Aquí, un sistema de caché bien diseñado es crucial para mantener la eficiencia, ya que todos los núcleos comparten el mismo espacio de memoria.

#### NUMA (Non-Uniform Memory Access)

En NUMA, la memoria se divide en varias secciones, cada una asociada con uno o varios procesadores específicos. Aunque un procesador puede acceder a toda la memoria, el acceso a la memoria "local" es más rápido que el acceso a la memoria "remota".

**Ejemplo NUMA**: Imagina un servidor de bases de datos con dos CPUs, cada una con su propia memoria RAM local. Si el Procesador A necesita acceder a datos en la memoria local del Procesador B, experimentará mayores latencias en comparación con el acceso a su propia memoria local. Este diseño permite escalar eficazmente sistemas con muchos procesadores, optimizando el rendimiento para aplicaciones específicas que pueden aprovechar la localidad de memoria.

### 6.3 Technical Details of Modern CPU and GPU Architectures

Las CPUs están diseñadas para maximizar el rendimiento por hilo de ejecución, con un énfasis en la velocidad de reloj y la eficiencia de las operaciones de un solo hilo. Las GPUs, por otro lado, se centran en el paralelismo masivo a costa de la frecuencia de reloj, lo que las hace ideales para computación científica y gráficos, donde se pueden realizar muchas operaciones en paralelo.

**Ejemplo de Arquitectura de CPU**: Los procesadores Intel Core i9 cuentan con tecnologías como Hyper-Threading y Turbo Boost para mejorar el rendimiento en tareas secuenciales y paralelas ligeras.

**Ejemplo de Arquitectura de GPU**: Las GPUs Nvidia RTX utilizan miles de núcleos CUDA organizados en SM (Streaming Multiprocessors) para ejecutar operaciones de flotantes y enteros en paralelo, lo que es ideal para el renderizado de gráficos y cálculos de aprendizaje profundo.

### Conclusión

Las secciones de la Parte 6 proporcionan una visión detallada de cómo el diseño arquitectónico de las computadoras afecta la programación y el rendimiento de los algoritmos paralelos, destacando la importancia de la selección de hardware adecuada para aplicaciones específicas en función de sus requisitos computacionales y patrones de acceso a memoria